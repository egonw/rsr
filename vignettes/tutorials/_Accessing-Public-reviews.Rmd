---
title: "Accessing-Public-Reviews"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Accessing-Public-Reviews}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

A case study extracting and modeling 

```{r eval=F, include=F}
library(tidyverse)
library(reticulate)
library(tensorflow)
library(keras)
library(magrittr)

reticulate::use_condaenv("r-reticulate",conda = "/home/thomas/miniconda3/bin/conda")

glove.lookup  <- (\(){
  gt = vroom(here("cache/glove.840B.300d.txt"),col_names = F,delim= ' ',quote='')
  gt |> mutate(tok=X1, arr=abind(gt[,-1]), glove.index=1:nrow(gt)) |> select(tok,arr,glove.index)
})()

numpy        = \(x){ x$numpy() }
keras_onehot = \(x){ array(as.numeric(x)) |> keras::k_one_hot(num_c=2) |> numpy() }
```

## Access data
```{r eval=F}
pid        = 78651   # Bisphenol PUBMED
pid        = 78647   # Raloxifen PUBMED
pid        = 101815  # Card Inju RIS

art.tb.raw = rsr::get_article_content(pid) |> unnest(content)
art.tb.raw 

art.tb = art.tb.raw |> 
  select(aid, TI=primary_title, AB=abstract) |>
  rowwise() |>  mutate(across(!aid, ~ paste0(.x,collapse=" "))) |> ungroup() |> 
  mutate(text = paste(TI,AB),.keep="unused")

ans.tb = rsr::get_answers(pid)                   |> 
  filter(short_label=="Include")                 |> 
  group_by(aid) |> slice(1) |> ungroup()         |> 
  mutate(answer = answer == "true")              |> 
  select(aid,answer)

sr.tb.raw = ans.tb |> inner_join(art.tb,by="aid")
```

## Tokenize
```{r eval=F}
build.glove.tokenizer = function(glove.lookup.in,text=NULL,padding=400){

  tokenizer  = \(x){ tokenizers::tokenize_ptb(x,lowercase = F) |> 
      map( ~ unlist(strsplit(.,"[.!?\\-]"))) }

  glove.lookup = if(is.null(text)){ glove.lookup.in }else{
    toks = tokenizer(text) |> unlist() |> unique()
    glove.lookup.in |> filter(tok %in% c(toks,"<unk>")) |> mutate(glove.index=row_number())
  } # shrink glove.lookup for text

  oov.i      = glove.lookup |> filter(tok == "<unk>") |> pull(glove.index)
  vocab.size = nrow(glove.lookup)
  pad.length = padding
  pad.seq    = function(seq,pad.length){ lapply(seq,function(s){s[1:pad.length] |> replace_na(0)}) }

  text_to_seq = function(text){
    tibble(tok = tokenizer(text)) |> mutate(row = row_number()) |> unnest(tok) |>
      left_join(glove.lookup,by="tok") |> replace_na( list(glove.index=oov.i) ) |>
      group_by(row) |> summarize(vec = list(glove.index)) |> ungroup() |>
      mutate(vec = pad.seq(vec,pad.length)) |>
      pull(vec) |>
      abind::abind(along = 0)
  }

  tibble::lst(glove.lookup, pad.length, vocab.size, text_to_seq)
}
```

```{r tokenizer-test, eval=F}
tok   = build.glove.tokenizer(glove.lookup,text = sr.tb.raw$text)

queen = tok$text_to_seq("queen")[1] %>% glove.lookup$arr[.,]
royal = tok$text_to_seq("royal")[1] %>% glove.lookup$arr[.,]
fetch = tok$text_to_seq("fetch")[1] %>% glove.lookup$arr[.,]

lsa::cosine(queen,royal)
lsa::cosine(queen,fetch)
```

## Train
```{r}
sr.tb = sr.tb.raw |> 
  mutate(out = keras_onehot(answer)) |>
  mutate(seq = tok$text_to_seq(text))  

# Sample in output groups
train.tb = sr.tb |> group_by(answer) |> sample_frac(0.8) |> ungroup() |> sample_frac(1.0)
test.tb  = sr.tb |> filter(!(aid %in% train.tb$aid)) |> sample_frac(1.0)

mod <- build.simple.lstm(tok) 
mod |> compile(loss     ="binary_crossentropy", # - log( P_true )
               optimizer="adam",
               metrics  ="accuracy")

mod |> fit(x=train.tb$seq, y=train.tb$out, epochs=1, batch=134)
keras::freeze_weights(mod$layers[[2]]) # freeze the embedding layer

mod |> fit(x=train.tb$seq, y=train.tb$out, batch=134, epochs=1,
           class_weight    = list(`0`=1,`1`=10),
           validation_data = list(test.tb$seq,test.tb$out))
```

## Evaluate
```{r}
library(pROC)
eval.tb = test.tb |> mutate(pred=predict(mod,seq)[,2])

ggplot(eval.tb,aes(pred,fill=answer,alpha=0.5)) + geom_density()

auc(eval.tb$answer, eval.tb$pred) |> round(dig=3)

ggroc(roc(eval.tb, answer, pred), colour = 'steelblue') + theme_minimal()

eval.tb |> filter(answer,pred<0.2) |> select(text)
```

## Embeddings
```{r}
embed.out = mod$get_layer("dense")$get_input_at(0L) 
embed.mod = keras_model(mod$inputs, embed.out)

res       = predict(embed.mod,sr.tb$seq)
distances = dist(res) |> as.matrix() 
arr       = array(distances)

edges.tb  = tibble(from=sr.tb$aid, to=sr.tb$aid) |> 
  complete(from, to) |> 
  arrange( from, to) |>  
  mutate(weights = arr) |> 
  filter(weights>0,weights < 0.25)

graph    = ForceAtlas2::layout.forceatlas2(edges.tb, directed=FALSE, 
                                           iterations = 1000, plotstep = 100)

graph.tb = graph |> mutate(aid = as.numeric(name),.keep="unused") |> tibble()

sr.graph = sr.tb |> inner_join(graph.tb,by="aid")

ggplot(sr.graph,aes(x=V1,y=V2,fill=answer,label=aid)) + geom_point(shape=21)
```

```{r}
sr.graph = readRDS(here::here("notes/sr.graph.RDS"))
ggplot(sr.graph,aes(x=V1,y=V2,fill=answer,label=aid)) + geom_point(shape=21)
plotly::ggplotly()

sr.tb |> 
  filter(aid %in% c(11626959,11626855,11628443,11626982)) |>
  select(text)
```





