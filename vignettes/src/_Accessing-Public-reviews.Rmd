---
title: "Accessing-Public-Reviews"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Accessing-Public-Reviews}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

A case study extracting and modeling 

```{r eval=F, include=F}
library(tidyverse)
library(reticulate)
library(tensorflow)
library(keras)
library(magrittr)

reticulate::use_condaenv("r-reticulate",conda = "/home/thomas/miniconda3/bin/conda")

# https://nlp.stanford.edu/data/glove.6B.zip
# glove = readLines(here("tmp/glove.6B.200d.txt")) 
# glove.lookup = purrr::map_dfr(glove,function(r){
#   spl.r = strsplit(r," ")[[1]]
#   tibble(tok = spl.r[1],arr = array(spl.r |> tail() |> as.numeric(),dim = c(1L,200L)))
# }) |> mutate(glove.index = row_number())
# saveRDS(glove.lookup,"~/tmp/glove.RDS")

glove.lookup = readRDS("~/tmp/glove.RDS")
numpy        = \(x){ x$numpy() }
keras_onehot = \(x){ array(as.numeric(x)) |> keras::k_one_hot(num_c=2) |> numpy() }
```

## Access data
```{r eval=F}
pid        = 78651   # Bisphenol PUBMED
pid        = 78647   # Raloxifen PUBMED
pid        = 101815  # Card Inju RIS

art.tb.raw = rsr::get_article_content(pid) |> unnest(content)
art.tb.raw 

art.tb = art.tb.raw |> 
  select(aid, TI=primary_title, AB=abstract) |>
  rowwise() |>  mutate(across(!aid, ~ paste0(.x,collapse=" "))) |> ungroup() |> 
  mutate(text = paste(TI,AB),.keep="unused")

ans.tb = rsr::get_answers(pid)                   |> 
  filter(short_label=="Include")                 |> 
  group_by(aid) |> slice(1) |> ungroup()         |> 
  mutate(answer = answer == "true")              |> 
  select(aid,answer)

sr.tb.raw = ans.tb |> inner_join(art.tb,by="aid")
```

## Tokenize
```{r eval=F}
build_tokenizer = function(){
  tokenizer         = tokenizers::tokenize_ptb
  vocab.size        = max(glove.lookup$glove.index)
  pad.length        = 400
  pad.seq           = function(seq,pad.length){ 
    lapply(seq,function(s){s[1:pad.length] |> replace_na(0)}) }

  text_to_seq = function(text){
    tibble(tok = tokenizer(text,lowercase =T)) |> mutate(row = row_number()) |> unnest(tok) |>
      left_join(glove.lookup,by="tok") |>
      replace_na(list(glove.index=max(glove.lookup$glove.index))) |>
      group_by(row) |> summarize(vec = list(glove.index)) |> ungroup() |>
      mutate(vec = pad.seq(vec,pad.length)) |>
      pull(vec) |>
      abind::abind(along = 0)
  }

  tibble::lst(glove.lookup, pad.length, vocab.size, text_to_seq)
}

```

```{r tokenizer-test, eval=F}
tok   = build_tokenizer()

queen = tok$text_to_seq("queen")[1] %>% glove.lookup$arr[.,]
royal = tok$text_to_seq("royal")[1] %>% glove.lookup$arr[.,]
fetch = tok$text_to_seq("fetch")[1] %>% glove.lookup$arr[.,]

lsa::cosine(queen,royal)
lsa::cosine(queen,fetch)
```

## Architect
```{r eval=F}
build_model = function(tok){
  input     <- layer_input(shape=c(tok$pad.length),name="input")
  embedding <- layer_embedding(input_dim    = tok$vocab.size,
                               weights      = list(tok$glove.lookup$arr),
                               output_dim   = dim(tok$glove.lookup$arr)[-1],
                               input_length = tok$pad.length,
                               mask_zero    = T,
                               trainable    = T,
                               name         = "text-embedding") # GLOVE EMBEDDING
  
  lstm      <- layer_lstm(units                 = 10,
                          activation            = "tanh",
                          recurrent_activation  = "sigmoid",
                          unroll = F,use_bias   = T,
                          # recurrent_regularizer = regularizer_l2(1e-4),
                          return_sequences      = F,
                          name                  = "tx-lstm") # SIMPLE LSTM
  
  bidir     <- embedding(input) |> bidirectional(layer = lstm) # FORWARD AND BACK

  output     = bidir |>
    # layer_batch_normalization() |>
    # layer_dropout(rate=0.3) |>
    layer_dense(2,
                # kernel_regularizer = regularizer_l2(1e-4),
                activation = "softmax") |>
    k_clip(min_value = 0.01,max_value = 0.99)
  
  keras_model(input,output) # PUT IT ALL TOGETHER
}
```

## Train
```{r}
sr.tb = sr.tb.raw |> 
  mutate(out = keras_onehot(answer)) |> # T -> [0 1]
  mutate(seq = tok$text_to_seq(text))   # tibble seqs

# Sample in output groups
train.tb = sr.tb |> group_by(answer) |> sample_frac(0.8) |> ungroup()
test.tb  = sr.tb |> filter(!(aid %in% train.tb$aid))

mod <- build_model(tok) 
mod |> compile(loss     ="binary_crossentropy", # - log( P_true )
               optimizer="adam")

mod |> fit(x=train.tb$seq, y=train.tb$out, epochs=1, batch=134)
keras::freeze_weights(mod$layers[[2]]) # freeze the embedding layer

mod |> fit(x=train.tb$seq, y=train.tb$out, batch=134, epochs=10,
           class_weight    = list(`0`=1,`1`=10),
           validation_data = list(test.tb$seq,test.tb$out))
```

## Evaluate
```{r}
library(pROC)
eval.tb = test.tb |> mutate(pred=predict(mod,seq)[,2])

ggplot(eval.tb,aes(pred,fill=answer,alpha=0.5)) + geom_density()

auc(eval.tb$answer, eval.tb$pred) |> round(dig=3)

ggroc(roc(eval.tb, answer, pred), colour = 'steelblue') + theme_minimal()

eval.tb |> filter(!answer,pred>0.8)
```

## Embeddings
```{r}
embed.out = mod$get_layer("dense")$get_input_at(0L) 
embed.mod = keras_model(mod$inputs, embed.out)

res       = predict(embed.mod,sr.tb$seq)
distances = dist(res) |> as.matrix() 
arr       = array(distances)

edges.tb  = tibble(from=sr.tb$aid, to=sr.tb$aid) |> 
  complete(from, to) |> 
  arrange( from, to) |>  
  mutate(weights = arr) |> 
  filter(weights>0,weights < 0.25)

graph    = ForceAtlas2::layout.forceatlas2(edges.tb, directed=FALSE, 
                                           iterations = 1000, plotstep = 100)

graph.tb = graph |> mutate(aid = as.numeric(name),.keep="unused") |> tibble()

sr.graph = sr.tb |> inner_join(graph.tb,by="aid")

ggplot(sr.graph,aes(x=V1,y=V2,fill=answer,label=aid)) + geom_point(shape=21)
```

```{r}
sr.graph = readRDS(here::here("notes/sr.graph.RDS"))
ggplot(sr.graph,aes(x=V1,y=V2,fill=answer,label=aid)) + geom_point(shape=21)
plotly::ggplotly()

sr.tb |> 
  filter(aid %in% c(11626959,11626855,11628443,11626982)) |>
  select(text)
```





